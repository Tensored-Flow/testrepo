"""
GitHub Integration â€” PR creation for approved optimizations.

TODO: Wire up with the frontend's Octokit/GitHub App auth.
The contract is simple: give it a RunResult, it creates a PR.
"""

from typing import Optional
from backend.models.types import RunResult, PipelineResult, Verdict


def create_optimization_pr(
    run_result: RunResult,
    owner: str,
    repo: str,
    base_branch: str = "main",
    github_token: Optional[str] = None,
) -> Optional[str]:
    """
    Create a GitHub PR with all approved optimizations from a pipeline run.

    Args:
        run_result: The completed pipeline result
        owner: GitHub repo owner
        repo: GitHub repo name
        base_branch: Branch to PR against
        github_token: GitHub API token (from GitHub App installation)

    Returns:
        PR URL if created, None if no approved optimizations

    TODO (teammate): Implement with PyGithub or requests.
    The frontend already has Octokit wired up â€” this is the backend equivalent.
    """
    approved = [
        r for r in run_result.results
        if r.status == "approved" and r.optimized_code
    ]

    if not approved:
        return None

    # Build PR body
    body = format_pr_body(approved)

    # TODO: Use GitHub API to:
    # 1. Create a new branch (e.g., "complexity-improver/{run_id}")
    # 2. For each approved result, update the file with optimized code
    # 3. Create the PR

    print(f"[GitHub] Would create PR with {len(approved)} optimizations")
    print(f"[GitHub] PR body preview:\n{body[:500]}")

    return None  # Replace with actual PR URL


def format_pr_body(approved_results: list[PipelineResult]) -> str:
    """Generate a markdown PR body summarizing all optimizations."""
    lines = [
        "## ðŸš€ Code Optimization by ComplexityImprover",
        "",
        "This PR contains AI-generated optimizations that have been **mathematically verified**:",
        "- âœ… All differential tests pass (same inputs â†’ same outputs)",
        "- âœ… No runtime or memory regressions",
        "- âœ… At least one metric improved",
        "- âœ… Statistical significance confirmed (p < 0.05)",
        "",
        "### Changes",
        "",
    ]

    for r in approved_results:
        v = r.validation
        h = r.hypothesis
        oc = r.optimized_code

        lines.append(f"#### `{r.function_name}()` in `{r.file_path}`")
        if h:
            lines.append(f"**Strategy:** {h.strategy}")
            lines.append(f"**Complexity:** {h.current_complexity} â†’ {h.proposed_complexity}")
        if v and v.improvements:
            improved = [m for m in v.improvements if m.improved]
            if improved:
                lines.append("**Improvements:**")
                for m in improved:
                    lines.append(f"- {m.metric_name}: {m.before:.2f} â†’ {m.after:.2f} ({m.delta_percent:+.1f}%)")
        lines.append(f"**Rounds:** {r.rounds_taken}")
        lines.append("")

    lines.extend([
        "---",
        "*Generated by [ComplexityImprover](https://github.com/hackeurope/complexity-improver) "
        "â€” AI proposes, math disposes.*",
    ])

    return "\n".join(lines)